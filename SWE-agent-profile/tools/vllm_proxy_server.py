#!/usr/bin/env python3
"""
vLLM ä»£ç†æœåŠ¡å™¨ - æ‹¦æˆªå¹¶è®°å½•æ‰€æœ‰å‘é€åˆ° vLLM çš„è¯·æ±‚

ç”¨æ³•ï¼š
    python tools/vllm_proxy_server.py --proxy-port 9000 --vllm-url http://localhost:8000

ç„¶åä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„ api_baseï¼š
    model:
        type: vllm
        api_base: http://localhost:9000/v1  # æŒ‡å‘ä»£ç†æœåŠ¡å™¨
"""

import argparse
import json
import time
from pathlib import Path
from typing import AsyncIterator

import aiohttp
import uvicorn
from fastapi import FastAPI, Request, Response
from fastapi.responses import StreamingResponse


class VLLMProxy:
    """vLLM ä»£ç†æœåŠ¡å™¨"""
    
    def __init__(self, vllm_url: str, log_dir: Path, verbose: bool = False):
        self.vllm_url = vllm_url.rstrip("/")
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        self.verbose = verbose
        self.request_count = 0
        
        # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
        self.request_log_file = self.log_dir / "requests.jsonl"
        self.response_log_file = self.log_dir / "responses.jsonl"
        
        print(f"âœ… vLLM ä»£ç†æœåŠ¡å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ“ æ—¥å¿—ç›®å½•: {self.log_dir}")
        print(f"   ğŸ¯ vLLM æœåŠ¡å™¨: {self.vllm_url}")
    
    def _log_to_file(self, filepath: Path, data: dict):
        """å†™å…¥æ—¥å¿—æ–‡ä»¶"""
        with open(filepath, "a", encoding="utf-8") as f:
            f.write(json.dumps(data, ensure_ascii=False) + "\n")
    
    def _print_request(self, request_data: dict):
        """æ‰“å°è¯·æ±‚ä¿¡æ¯"""
        if not self.verbose:
            return
        
        print(f"\n{'='*80}")
        print(f"[è¯·æ±‚ #{self.request_count}]")
        print(f"æ¨¡å‹: {request_data.get('model')}")
        print(f"Stream: {request_data.get('stream', False)}")
        print(f"æ¶ˆæ¯æ•°: {len(request_data.get('messages', []))}")
        
        # æ‰“å°æœ€åä¸€æ¡æ¶ˆæ¯ï¼ˆé€šå¸¸æ˜¯ç”¨æˆ·çš„é—®é¢˜ï¼‰
        messages = request_data.get('messages', [])
        if messages:
            last_msg = messages[-1]
            content = last_msg.get('content', '')
            if len(content) > 200:
                content = content[:200] + "..."
            print(f"æœ€åæ¶ˆæ¯: {last_msg.get('role')} -> {content}")
        print(f"{'='*80}\n")
    
    def _print_response(self, response_data: dict, duration: float):
        """æ‰“å°å“åº”ä¿¡æ¯"""
        if not self.verbose:
            return
        
        print(f"\n{'='*80}")
        print(f"[å“åº” #{self.request_count}]")
        print(f"è€—æ—¶: {duration:.2f}ç§’")
        
        if "choices" in response_data:
            for i, choice in enumerate(response_data["choices"]):
                message = choice.get("message", {})
                content = message.get("content", "")
                if len(content) > 200:
                    content = content[:200] + "..."
                print(f"é€‰æ‹© {i}: {content}")
        
        if "usage" in response_data:
            usage = response_data["usage"]
            print(f"Token ä½¿ç”¨: prompt={usage.get('prompt_tokens')}, "
                  f"completion={usage.get('completion_tokens')}, "
                  f"total={usage.get('total_tokens')}")
        print(f"{'='*80}\n")
    
    async def proxy_request(self, request: Request) -> Response:
        """ä»£ç†è¯·æ±‚åˆ° vLLM æœåŠ¡å™¨"""
        self.request_count += 1
        start_time = time.time()
        
        # è¯»å–è¯·æ±‚ä½“
        body = await request.body()
        request_data = json.loads(body) if body else {}
        
        # è®°å½•è¯·æ±‚
        log_entry = {
            "timestamp": start_time,
            "request_id": self.request_count,
            "method": request.method,
            "path": request.url.path,
            "request": request_data,
        }
        self._log_to_file(self.request_log_file, log_entry)
        self._print_request(request_data)
        
        # æ„é€ è½¬å‘çš„ URL
        target_url = f"{self.vllm_url}{request.url.path}"
        
        # åˆ¤æ–­æ˜¯å¦æ˜¯ stream æ¨¡å¼
        is_stream = request_data.get("stream", False)
        
        if is_stream:
            # Stream æ¨¡å¼ï¼šéœ€è¦ç‰¹æ®Šå¤„ç†
            return await self._proxy_stream_request(
                target_url, request, request_data, start_time
            )
        else:
            # é Stream æ¨¡å¼ï¼šç›´æ¥è½¬å‘
            return await self._proxy_normal_request(
                target_url, request, request_data, start_time
            )
    
    async def _proxy_normal_request(
        self, target_url: str, request: Request, request_data: dict, start_time: float
    ) -> Response:
        """ä»£ç†é stream è¯·æ±‚"""
        async with aiohttp.ClientSession() as session:
            # è½¬å‘è¯·æ±‚
            async with session.post(
                target_url,
                json=request_data,
                headers=dict(request.headers),
            ) as resp:
                response_body = await resp.read()
                response_data = json.loads(response_body)
                
                duration = time.time() - start_time
                
                # è®°å½•å“åº”
                log_entry = {
                    "timestamp": time.time(),
                    "request_id": self.request_count,
                    "duration": duration,
                    "response": response_data,
                }
                self._log_to_file(self.response_log_file, log_entry)
                self._print_response(response_data, duration)
                
                # è¿”å›å“åº”
                return Response(
                    content=response_body,
                    status_code=resp.status,
                    headers=dict(resp.headers),
                )
    
    async def _proxy_stream_request(
        self, target_url: str, request: Request, request_data: dict, start_time: float
    ) -> StreamingResponse:
        """ä»£ç† stream è¯·æ±‚"""
        
        async def stream_generator() -> AsyncIterator[bytes]:
            """ç”Ÿæˆå™¨ï¼šé€å—è½¬å‘å¹¶è®°å½•"""
            all_chunks = []
            accumulated_text = {}
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    target_url,
                    json=request_data,
                    headers=dict(request.headers),
                ) as resp:
                    # é€å—è¯»å–å’Œè½¬å‘
                    async for chunk in resp.content:
                        if not chunk:
                            continue
                        
                        # è®°å½• chunk
                        all_chunks.append(chunk)
                        
                        # è§£æ SSE æ ¼å¼
                        try:
                            # vLLM ä½¿ç”¨ SSE æ ¼å¼ï¼šdata: {...}\n\n
                            chunk_str = chunk.decode('utf-8')
                            for line in chunk_str.split('\n'):
                                if line.startswith('data: '):
                                    data_str = line[6:]  # å»æ‰ "data: "
                                    if data_str.strip() == '[DONE]':
                                        continue
                                    
                                    chunk_data = json.loads(data_str)
                                    choices = chunk_data.get("choices", [])
                                    
                                    for choice in choices:
                                        idx = choice.get("index", 0)
                                        delta = choice.get("delta", {})
                                        content = delta.get("content")
                                        if content:
                                            accumulated_text[idx] = accumulated_text.get(idx, "") + content
                        except Exception:
                            pass  # è§£æå¤±è´¥å°±è·³è¿‡
                        
                        # è½¬å‘åŸå§‹ chunk
                        yield chunk
            
            # Stream ç»“æŸï¼Œè®°å½•å®Œæ•´å“åº”
            duration = time.time() - start_time
            log_entry = {
                "timestamp": time.time(),
                "request_id": self.request_count,
                "duration": duration,
                "stream": True,
                "response": {
                    "num_chunks": len(all_chunks),
                    "accumulated_text": accumulated_text,
                },
            }
            self._log_to_file(self.response_log_file, log_entry)
            
            if self.verbose:
                print(f"\n{'='*80}")
                print(f"[Stream å“åº”å®Œæˆ #{self.request_count}]")
                print(f"è€—æ—¶: {duration:.2f}ç§’")
                print(f"æ€»å—æ•°: {len(all_chunks)}")
                for idx, text in accumulated_text.items():
                    preview = text[:200] + "..." if len(text) > 200 else text
                    print(f"æ–‡æœ¬ {idx}: {preview}")
                print(f"{'='*80}\n")
        
        return StreamingResponse(
            stream_generator(),
            media_type="text/event-stream",
        )


def create_app(vllm_url: str, log_dir: Path, verbose: bool = False) -> FastAPI:
    """åˆ›å»º FastAPI åº”ç”¨"""
    app = FastAPI(title="vLLM Proxy Server")
    proxy = VLLMProxy(vllm_url, log_dir, verbose)
    
    @app.get("/")
    async def root():
        return {
            "service": "vLLM Proxy Server",
            "vllm_url": proxy.vllm_url,
            "request_count": proxy.request_count,
        }
    
    @app.get("/health")
    async def health():
        return {"status": "ok"}
    
    @app.api_route("/{path:path}", methods=["GET", "POST", "PUT", "DELETE", "PATCH"])
    async def proxy_all(request: Request, path: str):
        """ä»£ç†æ‰€æœ‰è¯·æ±‚"""
        return await proxy.proxy_request(request)
    
    return app


def main():
    parser = argparse.ArgumentParser(description="vLLM ä»£ç†æœåŠ¡å™¨")
    parser.add_argument(
        "--proxy-port",
        type=int,
        default=9000,
        help="ä»£ç†æœåŠ¡å™¨ç›‘å¬ç«¯å£ï¼ˆé»˜è®¤: 9000ï¼‰",
    )
    parser.add_argument(
        "--proxy-host",
        type=str,
        default="0.0.0.0",
        help="ä»£ç†æœåŠ¡å™¨ç›‘å¬åœ°å€ï¼ˆé»˜è®¤: 0.0.0.0ï¼‰",
    )
    parser.add_argument(
        "--vllm-url",
        type=str,
        default="http://localhost:8000",
        help="vLLM æœåŠ¡å™¨åœ°å€ï¼ˆé»˜è®¤: http://localhost:8000ï¼‰",
    )
    parser.add_argument(
        "--log-dir",
        type=str,
        default="./proxy_logs",
        help="æ—¥å¿—ä¿å­˜ç›®å½•ï¼ˆé»˜è®¤: ./proxy_logsï¼‰",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="è¯¦ç»†è¾“å‡ºæ¨¡å¼",
    )
    
    args = parser.parse_args()
    
    print(f"""
{'='*80}
ğŸš€ å¯åŠ¨ vLLM ä»£ç†æœåŠ¡å™¨
{'='*80}
ç›‘å¬åœ°å€: {args.proxy_host}:{args.proxy_port}
vLLM åœ°å€: {args.vllm_url}
æ—¥å¿—ç›®å½•: {args.log_dir}
è¯¦ç»†è¾“å‡º: {args.verbose}
{'='*80}

é…ç½® SWE-agent ä½¿ç”¨ä»£ç†æœåŠ¡å™¨ï¼š
    model:
        api_base: http://localhost:{args.proxy_port}/v1

{'='*80}
    """)
    
    app = create_app(args.vllm_url, Path(args.log_dir), args.verbose)
    
    uvicorn.run(
        app,
        host=args.proxy_host,
        port=args.proxy_port,
        log_level="info",
    )


if __name__ == "__main__":
    main()

