vllm:
  model: /mnt/shared/models/GLM-4.6-FP8
  host: 0.0.0.0
  connect_host: 127.0.0.1
  port: 8000
  wait_timeout: 1200
  tokenizer: /mnt/shared/models/GLM-4.6-FP8
  gpu_memory_utilization: 0.8
  dtype: auto
  max_model_len: 131072
  extra_args:
  - --enable-auto-tool-choice
  - --enable-chunked-prefill
  - --enable-prefix-caching
  - --tool-call-parser
  - glm45
  - --max-num-batched-tokens
  - 65536
  - --tensor-parallel-size
  - 8
  env: {}
batch:
  sweagent_config: config/vllm_local.yaml
  args:
  - --instances.type
  - swe_bench
  - --instances.subset
  - lite
  - --instances.split
  - test
  runs:
  - label: bs1
    args:
    - --num_workers
    - 1
    - --suffix
    - bs1
  - label: bs4
    args:
    - --num_workers
    - 4
    - --suffix
    - bs4
  - label: bs8
    args:
    - --num_workers
    - 8
    - --suffix
    - bs8
  - label: bs16
    args:
    - --num_workers
    - 16
    - --suffix
    - bs16
  - label: bs32
    args:
    - --num_workers
    - 32
    - --suffix
    - bs32
  - label: bs48
    args:
    - --num_workers
    - 48
    - --suffix
    - bs48
  - label: bs64
    args:
    - --num_workers
    - 64
    - --suffix
    - bs64
  - label: bs72
    args:
    - --num_workers
    - 72
    - --suffix
    - bs72
  - label: bs96
    args:
    - --num_workers
    - 96
    - --suffix
    - bs96
  env: {}
