vllm:
  model: /data/models/SWE-agent-LM-32B
  host: 0.0.0.0
  connect_host: 127.0.0.1
  port: 8000
  wait_timeout: 240
  tokenizer: /data/models/SWE-agent-LM-32B
  gpu_memory_utilization: 0.8
  dtype: auto
  max_model_len: null
  extra_args:
    - --enable-auto-tool-choice
    - --enable-chunked-prefill
    - --enable-prefix-caching
    - --tool-call-parser
    - hermes
    - --max-num-batched-tokens
    - 1600
    - --max-num-seqs
    - 8

  disaggregated_prefill:
    wait_timeout: 240
    set_host_ip_env: true
    prefill:
      port: 8100
      cuda_visible_devices: "0,1"
      tensor_parallel: 2
      gpu_memory_utilization: 0.8
      extra_args:
        - --max-num-seqs
        - 8
      kv_transfer_config:
        kv_connector: P2pNcclConnector
        kv_role: kv_producer
        kv_rank: 0
        kv_parallel_size: 4
        kv_port: 21100
        kv_buffer_size: 4e9
        kv_connector_extra_config:
          http_port: "8100"
          proxy_ip: "127.0.0.1"
          proxy_port: "30001"
          send_type: PUT_ASYNC
          mem_pool_size_gb: 128
    decode:
      - port: 8200
        cuda_visible_devices: "2,3"
        tensor_parallel: 2
        gpu_memory_utilization: 0.75
        extra_args:
          - --max-num-seqs
          - 8
        kv_transfer_config:
          kv_connector: P2pNcclConnector
          kv_role: kv_consumer
          kv_rank: 1
          kv_parallel_size: 4
          kv_port: 22100
          kv_buffer_size: 1e10
          kv_connector_extra_config:
            http_port: "8200"
            proxy_ip: "127.0.0.1"
            proxy_port: "30001"
            send_type: PUT_ASYNC
            mem_pool_size_gb: 96

      - port: 8300
        cuda_visible_devices: "4,5"
        tensor_parallel: 2
        gpu_memory_utilization: 0.75
        extra_args:
          - --max-num-seqs
          - 8
        kv_transfer_config:
          kv_connector: P2pNcclConnector
          kv_role: kv_consumer
          kv_rank: 2
          kv_parallel_size: 4
          kv_port: 22104
          kv_buffer_size: 1e10
          kv_connector_extra_config:
            http_port: "8300"
            proxy_ip: "127.0.0.1"
            proxy_port: "30001"
            send_type: PUT_ASYNC
            mem_pool_size_gb: 96

      - port: 8400
        cuda_visible_devices: "6,7"
        tensor_parallel: 2
        gpu_memory_utilization: 0.75
        extra_args:
          - --max-num-seqs
          - 8
        kv_transfer_config:
          kv_connector: P2pNcclConnector
          kv_role: kv_consumer
          kv_rank: 3
          kv_parallel_size: 4
          kv_port: 22108
          kv_buffer_size: 1e10
          kv_connector_extra_config:
            http_port: "8400"
            proxy_ip: "127.0.0.1"
            proxy_port: "30001"
            send_type: PUT_ASYNC
            mem_pool_size_gb: 96

    proxy:
      port: 8000
    proxy:
      port: 8000

  env:
    PYTORCH_CUDA_ALLOC_CONF: expandable_segments:True

batch:
  sweagent_config: config/vllm_local_32B.yaml
  args:
    - --instances.type
    - swe_bench
    - --instances.subset
    - lite
    - --instances.split
    - test
    - --instances.slice
    - 64:128
  runs:
    - label: bs1
      args: [--num_workers, 1, --suffix, bs1]
    - label: bs4
      args: [--num_workers, 4, --suffix, bs4]
    - label: bs8
      args: [--num_workers, 8, --suffix, bs8]
    - label: bs16
      args: [--num_workers, 16, --suffix, bs16]
    - label: bs32
      args: [--num_workers, 32, --suffix, bs32]
    - label: bs64
      args: [--num_workers, 64, --suffix, bs64]
  env: {}
