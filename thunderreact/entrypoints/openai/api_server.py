#!/usr/bin/env python3
"""
ThunderReact OpenAI-compatible API Server
å®Œå…¨å…¼å®¹ vLLM çš„å¯åŠ¨æ–¹å¼ï¼Œè‡ªåŠ¨è½¬å‘æ‰€æœ‰è¯·æ±‚åˆ°çœŸå®çš„ vLLM æœåŠ¡å™¨
"""

import argparse
import asyncio
import json
import subprocess
import sys
import time
import signal
import socket
from pathlib import Path
from typing import AsyncIterator, Optional

import aiohttp
import uvicorn
from fastapi import FastAPI, Request, Response
from fastapi.responses import StreamingResponse


class ThunderReactProxy:
    """vLLM è¯·æ±‚ä»£ç†å’Œè®°å½•å™¨"""
    
    def __init__(self, vllm_port: int, log_dir: Path, verbose: bool = False):
        self.vllm_url = f"http://localhost:{vllm_port}"
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        self.verbose = verbose
        self.request_count = 0
        
        # æ—¥å¿—æ–‡ä»¶
        self.request_log_file = self.log_dir / "requests.jsonl"
        self.response_log_file = self.log_dir / "responses.jsonl"
        
        print(f"âœ… ThunderReact ä»£ç†æœåŠ¡å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ“ æ—¥å¿—ç›®å½•: {self.log_dir}")
        print(f"   ğŸ¯ è½¬å‘ç›®æ ‡: {self.vllm_url}")
    
    def _log_to_file(self, filepath: Path, data: dict):
        """å†™å…¥æ—¥å¿—"""
        with open(filepath, "a", encoding="utf-8") as f:
            f.write(json.dumps(data, ensure_ascii=False) + "\n")
    
    def _print_request(self, request_data: dict):
        """æ‰“å°è¯·æ±‚ä¿¡æ¯"""
        if not self.verbose:
            return
        
        print(f"\n{'='*80}")
        print(f"[ThunderReact è¯·æ±‚ #{self.request_count}]")
        print(f"æ¨¡å‹: {request_data.get('model')}")
        print(f"Stream: {request_data.get('stream', False)}")
        print(f"æ¶ˆæ¯æ•°: {len(request_data.get('messages', []))}")
        print(f"{'='*80}\n")
    
    def _print_response(self, response_data: dict, duration: float):
        """æ‰“å°å“åº”ä¿¡æ¯"""
        if not self.verbose:
            return
        
        print(f"\n{'='*80}")
        print(f"[ThunderReact å“åº” #{self.request_count}] è€—æ—¶: {duration:.2f}ç§’")
        if "usage" in response_data:
            usage = response_data["usage"]
            print(f"Token: prompt={usage.get('prompt_tokens')}, "
                  f"completion={usage.get('completion_tokens')}")
        print(f"{'='*80}\n")
    
    async def proxy_request(self, request: Request) -> Response:
        """ä»£ç†è¯·æ±‚åˆ° vLLM"""
        self.request_count += 1
        start_time = time.time()
        
        # è¯»å–è¯·æ±‚
        body = await request.body()
        request_data = json.loads(body) if body else {}
        
        # è®°å½•è¯·æ±‚
        log_entry = {
            "timestamp": start_time,
            "request_id": self.request_count,
            "method": request.method,
            "path": request.url.path,
            "request": request_data,
        }
        self._log_to_file(self.request_log_file, log_entry)
        self._print_request(request_data)
        
        # è½¬å‘ URL
        target_url = f"{self.vllm_url}{request.url.path}"
        
        # åˆ¤æ–­æ˜¯å¦ stream
        is_stream = request_data.get("stream", False)
        
        if is_stream:
            return await self._proxy_stream(target_url, request, request_data, start_time)
        else:
            return await self._proxy_normal(target_url, request, request_data, start_time)
    
    async def _proxy_normal(
        self, target_url: str, request: Request, request_data: dict, start_time: float
    ) -> Response:
        """ä»£ç†æ™®é€šè¯·æ±‚"""
        async with aiohttp.ClientSession() as session:
            async with session.post(
                target_url,
                json=request_data,
                headers=dict(request.headers),
            ) as resp:
                response_body = await resp.read()
                response_data = json.loads(response_body)
                
                duration = time.time() - start_time
                
                # è®°å½•å“åº”
                log_entry = {
                    "timestamp": time.time(),
                    "request_id": self.request_count,
                    "duration": duration,
                    "response": response_data,
                }
                self._log_to_file(self.response_log_file, log_entry)
                self._print_response(response_data, duration)
                
                return Response(
                    content=response_body,
                    status_code=resp.status,
                    headers=dict(resp.headers),
                )
    
    async def _proxy_stream(
        self, target_url: str, request: Request, request_data: dict, start_time: float
    ) -> StreamingResponse:
        """ä»£ç† stream è¯·æ±‚"""
        
        async def stream_generator() -> AsyncIterator[bytes]:
            all_chunks = []
            accumulated_text = {}
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    target_url,
                    json=request_data,
                    headers=dict(request.headers),
                ) as resp:
                    async for chunk in resp.content:
                        if not chunk:
                            continue
                        
                        all_chunks.append(chunk)
                        
                        # è§£æ SSE
                        try:
                            chunk_str = chunk.decode('utf-8')
                            for line in chunk_str.split('\n'):
                                if line.startswith('data: '):
                                    data_str = line[6:]
                                    if data_str.strip() == '[DONE]':
                                        continue
                                    chunk_data = json.loads(data_str)
                                    choices = chunk_data.get("choices", [])
                                    for choice in choices:
                                        idx = choice.get("index", 0)
                                        delta = choice.get("delta", {})
                                        content = delta.get("content")
                                        if content:
                                            accumulated_text[idx] = accumulated_text.get(idx, "") + content
                        except Exception:
                            pass
                        
                        # è½¬å‘åŸå§‹ chunk
                        yield chunk
            
            # è®°å½•å®Œæ•´å“åº”
            duration = time.time() - start_time
            log_entry = {
                "timestamp": time.time(),
                "request_id": self.request_count,
                "duration": duration,
                "stream": True,
                "response": {
                    "num_chunks": len(all_chunks),
                    "accumulated_text": accumulated_text,
                },
            }
            self._log_to_file(self.response_log_file, log_entry)
            
            if self.verbose:
                print(f"\n[ThunderReact Stream #{self.request_count}] å®Œæˆï¼Œè€—æ—¶ {duration:.2f}ç§’\n")
        
        return StreamingResponse(
            stream_generator(),
            media_type="text/event-stream",
        )


def create_app(vllm_port: int, log_dir: Path, verbose: bool) -> FastAPI:
    """åˆ›å»º FastAPI åº”ç”¨"""
    app = FastAPI(title="ThunderReact OpenAI-compatible API")
    proxy = ThunderReactProxy(vllm_port, log_dir, verbose)
    
    @app.get("/")
    async def root():
        return {
            "service": "ThunderReact Proxy",
            "vllm_url": proxy.vllm_url,
            "request_count": proxy.request_count,
        }
    
    @app.get("/health")
    async def health():
        return {"status": "ok"}
    
    # æ•è·æ‰€æœ‰å…¶ä»–è·¯ç”±å¹¶è½¬å‘
    @app.get("/{path:path}")
    async def proxy_get(request: Request, path: str):
        return await proxy.proxy_request(request)
    
    @app.post("/{path:path}")
    async def proxy_post(request: Request, path: str):
        return await proxy.proxy_request(request)
    
    @app.put("/{path:path}")
    async def proxy_put(request: Request, path: str):
        return await proxy.proxy_request(request)
    
    @app.delete("/{path:path}")
    async def proxy_delete(request: Request, path: str):
        return await proxy.proxy_request(request)
    
    @app.patch("/{path:path}")
    async def proxy_patch(request: Request, path: str):
        return await proxy.proxy_request(request)
    
    return app


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="ThunderReact OpenAI-compatible API Server (vLLM Proxy)",
        add_help=False,  # ç¦ç”¨é»˜è®¤ helpï¼Œé¿å…å’Œ vLLM å‚æ•°å†²çª
    )
    
    # ThunderReact ç‰¹æœ‰å‚æ•°
    parser.add_argument("--host", type=str, default="0.0.0.0", help="ä»£ç†æœåŠ¡å™¨ç›‘å¬åœ°å€")
    parser.add_argument("--port", type=int, default=9000, help="ä»£ç†æœåŠ¡å™¨ç›‘å¬ç«¯å£")
    parser.add_argument(
        "--vllm-port",
        type=int,
        default=8000,
        help="vLLM æœåŠ¡å™¨ç«¯å£ï¼ˆé»˜è®¤: 8000ï¼‰",
    )
    parser.add_argument(
        "--log-dir",
        type=str,
        default="./thunderreact_logs",
        help="æ—¥å¿—ä¿å­˜ç›®å½•",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="è¯¦ç»†è¾“å‡ºæ¨¡å¼",
    )
    parser.add_argument(
        "--auto-start-vllm",
        action="store_true",
        help="è‡ªåŠ¨åœ¨å†…éƒ¨å¯åŠ¨ vLLM æœåŠ¡å™¨",
    )
    parser.add_argument(
        "--help",
        action="store_true",
        help="æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯",
    )
    
    # è§£æå·²çŸ¥å‚æ•°ï¼Œä¿ç•™æœªçŸ¥å‚æ•°
    args, unknown = parser.parse_known_args()
    
    if args.help:
        parser.print_help()
        print("\næ‰€æœ‰å…¶ä»–å‚æ•°å°†ä¼ é€’ç»™ vLLMï¼ˆå¦‚æœ --auto-start-vllm å¯ç”¨ï¼‰")
        sys.exit(0)
    
    # ä¿å­˜æ‰€æœ‰åŸå§‹å‚æ•°å’Œ vLLM å‚æ•°
    args.vllm_args = unknown
    args.all_args = sys.argv[1:]  # ä¿å­˜æ‰€æœ‰åŸå§‹å‚æ•°
    
    return args


def build_vllm_command(args) -> list[str]:
    """æ„å»º vLLM å¯åŠ¨å‘½ä»¤"""
    # è¿‡æ»¤æ‰ ThunderReact ç‰¹æœ‰çš„å‚æ•°
    thunderreact_args = {
        '--vllm-port', '--log-dir', '--verbose', '--auto-start-vllm'
    }
    
    vllm_cmd = ['python', '-m', 'vllm.entrypoints.openai.api_server']
    
    # æ·»åŠ æ‰€æœ‰å‚æ•°ï¼Œä½†æ’é™¤ ThunderReact ç‰¹æœ‰å‚æ•°
    i = 0
    all_args = args.all_args
    while i < len(all_args):
        arg = all_args[i]
        
        # è·³è¿‡ ThunderReact å‚æ•°
        if arg in thunderreact_args:
            i += 1
            # å¦‚æœä¸‹ä¸€ä¸ªå‚æ•°ä¸æ˜¯ä»¥ -- å¼€å¤´ï¼Œè¯´æ˜æ˜¯è¿™ä¸ªå‚æ•°çš„å€¼ï¼Œä¹Ÿè·³è¿‡
            if i < len(all_args) and not all_args[i].startswith('--'):
                i += 1
            continue
        
        # å¤„ç† --portï¼Œæ”¹æˆ --vllm-port æŒ‡å®šçš„ç«¯å£
        if arg == '--port':
            vllm_cmd.append('--port')
            vllm_cmd.append(str(args.vllm_port))
            i += 1
            # è·³è¿‡åŸæ¥çš„ç«¯å£å€¼
            if i < len(all_args) and not all_args[i].startswith('--'):
                i += 1
            continue
        
        # å…¶ä»–å‚æ•°ç›´æ¥æ·»åŠ 
        vllm_cmd.append(arg)
        i += 1
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®š --portï¼Œæ·»åŠ é»˜è®¤ç«¯å£
    if '--port' not in vllm_cmd:
        vllm_cmd.extend(['--port', str(args.vllm_port)])
    
    return vllm_cmd


def wait_for_vllm_ready(port: int, timeout: int = 300) -> bool:
    """ç­‰å¾… vLLM æœåŠ¡å™¨å¯åŠ¨å®Œæˆ"""
    print(f"â³ ç­‰å¾… vLLM æœåŠ¡å™¨åœ¨ç«¯å£ {port} å¯åŠ¨...")
    start_time = time.time()
    
    while time.time() - start_time < timeout:
        try:
            # å°è¯•è¿æ¥ç«¯å£
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(1)
            result = sock.connect_ex(('localhost', port))
            sock.close()
            
            if result == 0:
                # ç«¯å£å¼€æ”¾ï¼Œå°è¯•è®¿é—® /v1/models
                try:
                    import requests
                    response = requests.get(f"http://localhost:{port}/v1/models", timeout=5)
                    if response.status_code == 200:
                        print(f"âœ… vLLM æœåŠ¡å™¨å·²å°±ç»ªï¼")
                        return True
                except Exception:
                    pass
        except Exception:
            pass
        
        time.sleep(2)
        print(".", end="", flush=True)
    
    print(f"\nâŒ vLLM æœåŠ¡å™¨åœ¨ {timeout} ç§’å†…æœªèƒ½å¯åŠ¨")
    return False


def main():
    args = parse_args()
    
    vllm_process: Optional[subprocess.Popen] = None
    
    try:
        if args.auto_start_vllm:
            # è‡ªåŠ¨å¯åŠ¨ vLLM
            print(f"""
{'='*80}
ğŸš€ ThunderReact - è‡ªåŠ¨å¯åŠ¨æ¨¡å¼
{'='*80}
ä»£ç†æœåŠ¡å™¨: {args.host}:{args.port}
vLLM ç«¯å£: {args.vllm_port}
æ—¥å¿—ç›®å½•: {args.log_dir}
{'='*80}
            """)
            
            # æ„å»º vLLM å‘½ä»¤
            vllm_cmd = build_vllm_command(args)
            print(f"ğŸ”§ å¯åŠ¨ vLLM æœåŠ¡å™¨...")
            if args.verbose:
                print(f"   å‘½ä»¤: {' '.join(vllm_cmd)}")
            
            # å¯åŠ¨ vLLM è¿›ç¨‹
            vllm_process = subprocess.Popen(
                vllm_cmd,
                stdout=subprocess.PIPE if not args.verbose else None,
                stderr=subprocess.PIPE if not args.verbose else None,
            )
            
            # ç­‰å¾… vLLM å¯åŠ¨
            if not wait_for_vllm_ready(args.vllm_port):
                if vllm_process:
                    vllm_process.terminate()
                    vllm_process.wait()
                print("âŒ vLLM å¯åŠ¨å¤±è´¥")
                sys.exit(1)
        else:
            # æ‰‹åŠ¨æ¨¡å¼
            print(f"""
{'='*80}
ğŸš€ ThunderReact OpenAI-compatible API Server
{'='*80}
ç›‘å¬åœ°å€: {args.host}:{args.port}
è½¬å‘ç›®æ ‡: http://localhost:{args.vllm_port}
æ—¥å¿—ç›®å½•: {args.log_dir}
è¯¦ç»†æ¨¡å¼: {args.verbose}
{'='*80}

âš ï¸  è¯·ç¡®ä¿ vLLM æœåŠ¡å™¨å·²åœ¨ç«¯å£ {args.vllm_port} ä¸Šè¿è¡Œï¼

ä½¿ç”¨æ–¹å¼ï¼š
    1. å…ˆå¯åŠ¨ vLLM æœåŠ¡å™¨åœ¨ç«¯å£ {args.vllm_port}
    2. æœ¬æœåŠ¡å™¨ä¼šè‡ªåŠ¨è½¬å‘æ‰€æœ‰è¯·æ±‚
    3. æŸ¥çœ‹æ—¥å¿—: {args.log_dir}/requests.jsonl å’Œ responses.jsonl

{'='*80}
            """)
        
        # å¯åŠ¨ä»£ç†æœåŠ¡å™¨
        print(f"ğŸŒ å¯åŠ¨ä»£ç†æœåŠ¡å™¨åœ¨ {args.host}:{args.port}")
        app = create_app(args.vllm_port, Path(args.log_dir), args.verbose)
        
        uvicorn.run(
            app,
            host=args.host,
            port=args.port,
            log_level="info",
        )
    
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨å…³é—­...")
    
    finally:
        # æ¸…ç†ï¼šå…³é—­ vLLM è¿›ç¨‹
        if vllm_process:
            print("ğŸ›‘ æ­£åœ¨åœæ­¢ vLLM æœåŠ¡å™¨...")
            vllm_process.terminate()
            try:
                vllm_process.wait(timeout=10)
            except subprocess.TimeoutExpired:
                print("âš ï¸  vLLM æœªèƒ½æ­£å¸¸åœæ­¢ï¼Œå¼ºåˆ¶ç»ˆæ­¢...")
                vllm_process.kill()
                vllm_process.wait()
            print("âœ… vLLM æœåŠ¡å™¨å·²åœæ­¢")
        
        print("âœ… ThunderReact å·²å®Œå…¨åœæ­¢")
        sys.exit(0)


if __name__ == "__main__":
    main()

